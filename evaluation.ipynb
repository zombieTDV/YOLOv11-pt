{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba726dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_37236\\2013043278.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 5000/5000 [04:08<00:00, 20.13img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 24767 predictions to yolov11_predictions.json (processed 5000 images)\n",
      "Loading and preparing results...\n",
      "DONE (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=10.85s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.72s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.443\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.355\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.352\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.371\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.373\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.131\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.593\n"
     ]
    }
   ],
   "source": [
    "# evaluate_yolov11_final.py\n",
    "# Full evaluation script that mirrors your working inference pipeline (letterbox, dtype handling, scaling).\n",
    "# Saves COCO-format predictions and runs COCOeval.\n",
    "#\n",
    "# Usage: place in repo root and run: python evaluate_yolov11_final.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "WEIGHT = \"weights/best.pt\"\n",
    "IMG_DIR = \"dataset/coco/val2017\"                       # folder with images (must match COCO file_name)\n",
    "ANN_FILE = \"annotations/instances_val2017.json\"\n",
    "IMG_SIZE = (640, 640)   # (H, W) as used by your model/training\n",
    "CONF_THR = 0.25\n",
    "IOU_THR = 0.45\n",
    "SAVE_JSON = \"yolov11_predictions.json\"\n",
    "VISUALIZE = False        # True -> save a few debug images (vis_<imgid>.jpg)\n",
    "# ----------------\n",
    "\n",
    "# COCO 0..79 -> 1..90 mapping (use only if annotations require it)\n",
    "COCO91CLASS = {\n",
    "    0:1,1:2,2:3,3:4,4:5,5:6,6:7,7:8,8:9,9:10,\n",
    "    10:11,11:13,12:14,13:15,14:16,15:17,16:18,17:19,18:20,\n",
    "    19:21,20:22,21:23,22:24,23:25,24:27,25:28,26:31,27:32,\n",
    "    28:33,29:34,30:35,31:36,32:37,33:38,34:39,35:40,36:41,\n",
    "    37:42,38:43,39:44,40:46,41:47,42:48,43:49,44:50,45:51,\n",
    "    46:52,47:53,48:54,49:55,50:56,51:57,52:58,53:59,54:60,\n",
    "    55:61,56:62,57:63,58:64,59:65,60:67,61:70,62:72,63:73,\n",
    "    64:74,65:75,66:76,67:77,68:78,69:79,70:80,71:81,72:82,\n",
    "    73:84,74:85,75:86,76:87,77:88,78:89,79:90\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- helpers (reuse exact letterbox from your working inference) ----\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    \"\"\"Resize and pad image (returns padded_img, gain, (pad_left, pad_top)).\"\"\"\n",
    "    h0, w0 = img.shape[:2]\n",
    "    new_h, new_w = new_shape\n",
    "    r = min(new_h / h0, new_w / w0)\n",
    "    new_unpad_w = int(round(w0 * r))\n",
    "    new_unpad_h = int(round(h0 * r))\n",
    "    img_resized = cv2.resize(img, (new_unpad_w, new_unpad_h), interpolation=cv2.INTER_LINEAR)\n",
    "    dw = new_w - new_unpad_w\n",
    "    dh = new_h - new_unpad_h\n",
    "    top = int(round(dh / 2 - 0.1))\n",
    "    bottom = int(round(dh / 2 + 0.1))\n",
    "    left = int(round(dw / 2 - 0.1))\n",
    "    right = int(round(dw / 2 + 0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right,\n",
    "                                    cv2.BORDER_CONSTANT, value=color)\n",
    "    return img_padded, r, (left, top)\n",
    "\n",
    "def scale_coords_from_padded(dets, gain, pad, orig_w, orig_h):\n",
    "    \"\"\"\n",
    "    dets: tensor Nx6 (x1,y1,x2,y2,conf,cls) in padded image coordinates\n",
    "    gain: scaling gain used for resize\n",
    "    pad: (pad_left, pad_top) values returned by letterbox\n",
    "    \"\"\"\n",
    "    # operate inplace on a clone outside if needed\n",
    "    dets = dets.clone()\n",
    "    pad_left, pad_top = pad\n",
    "    # remove padding\n",
    "    dets[:, [0, 2]] -= pad_left\n",
    "    dets[:, [1, 3]] -= pad_top\n",
    "    # divide by gain (undo scaling)\n",
    "    dets[:, :4] /= gain\n",
    "    # clamp\n",
    "    dets[:, [0, 2]] = dets[:, [0, 2]].clamp(0, orig_w)\n",
    "    dets[:, [1, 3]] = dets[:, [1, 3]].clamp(0, orig_h)\n",
    "    return dets\n",
    "\n",
    "# ---- import repo NMS (assumes utils/util.py in repo) ----\n",
    "try:\n",
    "    from utils.util import non_max_suppression  # expects model-native output shape [B, C, N]\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Failed to import repo non_max_suppression from utils.util: \" + str(e))\n",
    "\n",
    "# ---- main evaluation ----\n",
    "def main():\n",
    "    # load model from checkpoint exactly as your inference code does\n",
    "    ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n",
    "    if 'model' not in ckpt:\n",
    "        raise RuntimeError(\"Checkpoint does not contain 'model' key.\")\n",
    "    model = ckpt['model']\n",
    "    model.eval()\n",
    "\n",
    "    # dtype handling (match inference)\n",
    "    if next(model.parameters()).dtype == torch.half:\n",
    "        model = model.half()\n",
    "    else:\n",
    "        model = model.float()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    cocoGt = COCO(ANN_FILE)\n",
    "    img_ids = cocoGt.getImgIds()\n",
    "\n",
    "    results = []\n",
    "    processed = 0\n",
    "    debug_printed = 0\n",
    "\n",
    "    for img_id in tqdm(img_ids, desc=\"Evaluating\", unit=\"img\"):\n",
    "        info = cocoGt.loadImgs(img_id)[0]\n",
    "        file_name = info['file_name']\n",
    "        img_path = os.path.join(IMG_DIR, file_name)\n",
    "        if not os.path.exists(img_path):\n",
    "            # skip if image not available locally\n",
    "            continue\n",
    "\n",
    "        img_bgr = cv2.imread(img_path)\n",
    "        if img_bgr is None:\n",
    "            continue\n",
    "        orig_h, orig_w = img_bgr.shape[:2]\n",
    "\n",
    "        # preprocessing (exact same as your working inference)\n",
    "        img_pad, gain, (pad_w, pad_h) = letterbox(img_bgr, new_shape=IMG_SIZE)\n",
    "        img_rgb = cv2.cvtColor(img_pad, cv2.COLOR_BGR2RGB)\n",
    "        img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "        img_tensor = img_tensor.to(device)\n",
    "        if next(model.parameters()).dtype == torch.half:\n",
    "            img_tensor = img_tensor.half()\n",
    "\n",
    "        # forward (use model output as-is, repo NMS expects that)\n",
    "        with torch.no_grad():\n",
    "            out = model(img_tensor)\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = out[0]   # sometimes (pred, loss)\n",
    "\n",
    "        # out expected shape: [B, C, N] (like your inference)\n",
    "        if not torch.is_tensor(out):\n",
    "            continue\n",
    "\n",
    "        # Ensure class logits are probabilities: if raw logits exceed 1 apply sigmoid to class slice\n",
    "        # C = out.shape[1], nc = C - 4\n",
    "        try:\n",
    "            C = out.shape[1]\n",
    "            nc = C - 4\n",
    "            # check a small slice to decide if logits present\n",
    "            cls_slice = out[:, 4:4+nc]\n",
    "            if float(cls_slice.max()) > 1.0:\n",
    "                out[:, 4:4+nc] = cls_slice.sigmoid()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Run repo NMS (works with output as-is)\n",
    "        dets = non_max_suppression(out, confidence_threshold=CONF_THR, iou_threshold=IOU_THR)[0]\n",
    "\n",
    "        if dets is None or len(dets) == 0:\n",
    "            processed += 1\n",
    "            continue\n",
    "\n",
    "        # Map boxes from padded input -> original image exactly like inference\n",
    "        dets = dets.detach().cpu()\n",
    "        dets = scale_coords_from_padded(dets, gain, (pad_w, pad_h), orig_w, orig_h)\n",
    "\n",
    "        # collect results (COCO expects [x,y,w,h] and category IDs must match annotation file)\n",
    "        # Decide if mapping to COCO91 needed:\n",
    "        gt_cat_ids = set([c['id'] for c in cocoGt.loadCats(cocoGt.getCatIds())])\n",
    "        mapping_needed = (0 not in gt_cat_ids)  # if GT IDs are 1..90, map from 0..79 to 1..90\n",
    "\n",
    "        for *xyxy, conf, cls in dets:\n",
    "            x1, y1, x2, y2 = [float(v) for v in xyxy]\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "\n",
    "            cls_idx = int(cls.item())  # YOLO class index (0..79)\n",
    "            if mapping_needed:\n",
    "                coco_cat_id = COCO91CLASS.get(cls_idx)\n",
    "                if coco_cat_id is None:\n",
    "                    continue\n",
    "            else:\n",
    "                coco_cat_id = cls_idx\n",
    "\n",
    "            score = float(conf.item())\n",
    "            # clamp score to [0,1]\n",
    "            score = max(0.0, min(1.0, score))\n",
    "\n",
    "            results.append({\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": int(coco_cat_id),\n",
    "                \"bbox\": [max(0.0, x1), max(0.0, y1), float(w), float(h)],\n",
    "                \"score\": score\n",
    "            })\n",
    "\n",
    "        # optional visualization of one image (quick sanity)\n",
    "        if VISUALIZE and processed % 10 == 0:\n",
    "            vis = img_bgr.copy()\n",
    "            for *xyxy, conf, cls in dets:\n",
    "                xa, ya, xb, yb = map(int, xyxy)\n",
    "                cv2.rectangle(vis, (xa, ya), (xb, yb), (0,255,0), 2)\n",
    "                cv2.putText(vis, f\"{int(cls.item())}:{conf.item():.2f}\", (xa, max(0, ya-6)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "            cv2.imwrite(f\"vis_{img_id}.jpg\", vis)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    # save predictions\n",
    "    with open(SAVE_JSON, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "    print(f\"\\nSaved {len(results)} predictions to {SAVE_JSON} (processed {processed} images)\")\n",
    "\n",
    "    # run COCOeval\n",
    "    if len(results) == 0:\n",
    "        print(\"No results to evaluate.\")\n",
    "        return\n",
    "\n",
    "    cocoDt = cocoGt.loadRes(SAVE_JSON)\n",
    "    cocoEval = COCOeval(cocoGt, cocoDt, \"bbox\")\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a818f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f289c",
   "metadata": {},
   "source": [
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.325\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.443\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.355\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.119\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.352\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.512\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.269\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.371\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.373\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.131\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.400\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.593"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
