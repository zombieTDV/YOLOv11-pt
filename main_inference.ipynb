{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c94d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_letterbox.py\n",
    "import torch, cv2, numpy as np\n",
    "from utils.util import non_max_suppression\n",
    "import torchvision.transforms as T\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55ebcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- params ----------\n",
    "WEIGHT = \"weights/best.pt\"\n",
    "IMG_PATH = \"dataset/VietNam_street.png\"   # change if needed\n",
    "DATA_YAML = \"utils/args.yaml\"   # Path to dataset yaml\n",
    "INPUT_SIZE = (640, 640)          # height, width used by model\n",
    "CONF_THR = 0.25\n",
    "IOU_THR = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f10f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_YAML, \"r\") as f:\n",
    "    data_dict = yaml.safe_load(f)\n",
    "\n",
    "names = data_dict[\"names\"]   # dict {0:\"person\",1:\"bicycle\",...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3da5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utils ----------\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # img: BGR numpy (H, W, C) as loaded by cv2\n",
    "    h0, w0 = img.shape[:2]\n",
    "    new_h, new_w = new_shape\n",
    "    r = min(new_h / h0, new_w / w0)\n",
    "    new_unpad_w = int(round(w0 * r))\n",
    "    new_unpad_h = int(round(h0 * r))\n",
    "    # resize\n",
    "    img_resized = cv2.resize(img, (new_unpad_w, new_unpad_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # compute padding\n",
    "    dw = new_w - new_unpad_w\n",
    "    dh = new_h - new_unpad_h\n",
    "    top = int(round(dh / 2 - 0.1))\n",
    "    bottom = int(round(dh / 2 + 0.1))\n",
    "    left = int(round(dw / 2 - 0.1))\n",
    "    right = int(round(dw / 2 + 0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right,\n",
    "                                    cv2.BORDER_CONSTANT, value=color)\n",
    "    return img_padded, r, (left, top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04361af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_9576\\1903992144.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# ---------- load model ----------\n",
    "ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n",
    "if 'model' in ckpt:\n",
    "    model = ckpt['model']\n",
    "else:\n",
    "    raise RuntimeError(\"Checkpoint does not contain 'model' key.\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# prefer float for numeric stability\n",
    "if next(model.parameters()).dtype == torch.half:\n",
    "    model = model.half()\n",
    "else:\n",
    "    model = model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e765ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- load + preprocess with letterbox ----------\n",
    "# read with cv2 (BGR), convert to RGB only where needed\n",
    "img_bgr = cv2.imread(IMG_PATH) #np.array (H, W, 3)\n",
    "if img_bgr is None:\n",
    "    raise FileNotFoundError(f\"Image not found: {IMG_PATH}\")\n",
    "orig_h, orig_w = img_bgr.shape[:2] #np.array (H, W)\n",
    "\n",
    "img_pad, gain, (pad_w, pad_h) = letterbox(img_bgr, new_shape=INPUT_SIZE) \n",
    "'''\n",
    "Gọi hàm letterbox để resize ảnh về kích thước chuẩn của YOLO (INPUT_SIZE, ví dụ 640×640).\n",
    "\n",
    "Letterbox = resize ảnh nhưng vẫn giữ tỉ lệ khung hình (aspect ratio) → phần thừa sẽ được padding màu đen.\n",
    "Trả về:\n",
    "\n",
    "img_pad: ảnh sau khi resize + pad.\n",
    "\n",
    "gain: hệ số scale (ảnh gốc → ảnh mới).\n",
    "\n",
    "(pad_w, pad_h): độ pad thêm ở 2 chiều.\n",
    "\n",
    "Thông tin gain, pad_w, pad_h được dùng sau này để chuyển ngược bbox từ ảnh YOLO về ảnh gốc.\n",
    "'''\n",
    "# convert to RGB floating tensor [1,3,H,W] in 0..1\n",
    "img_rgb = cv2.cvtColor(img_pad, cv2.COLOR_BGR2RGB) #Chuyển từ BGR → RGB (YOLO và PyTorch thường chuẩn hóa input thành RGB).\n",
    "img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "'''\n",
    "torch.from_numpy(img_rgb): chuyển ảnh NumPy → Tensor PyTorch.\n",
    "\n",
    ".permute(2, 0, 1): đổi trục từ (H, W, C) → (C, H, W) (PyTorch format).\n",
    "\n",
    ".unsqueeze(0): thêm batch dimension → [1, C, H, W].\n",
    "\n",
    ".float() / 255.0: đổi từ uint8 (0–255) sang float32 (0–1) để mạng dễ học\n",
    "'''\n",
    "img_tensor = img_tensor.to(device)\n",
    "if next(model.parameters()).dtype == torch.half:\n",
    "    img_tensor = img_tensor.half()\n",
    "'''\n",
    "    Nếu mô hình đang ở dạng half precision (FP16) → convert input sang .half() để đồng bộ.\n",
    "\n",
    "Dùng khi inference trên GPU để tăng tốc và tiết kiệm bộ nhớ.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d92f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw output shape: torch.Size([1, 84, 8400])\n",
      "Before scale: x_min, y_min, x_max, y_max ranges: 35.4375 153.25 640.5 529.5\n",
      "Sample raw detection (first 5):\n",
      "0 [365.75      313.        456.25      498.          0.8540039   0.       ]\n",
      "1 [303.5        366.         541.         507.           0.85058594\n",
      "   3.        ]\n",
      "2 [175.5     311.25    317.75    410.25      0.84375   2.     ]\n",
      "3 [323.5       301.5       345.5       364.5         0.7441406   0.       ]\n",
      "4 [571.        284.5       640.        441.          0.7241211   0.       ]\n",
      "After scale (to original image): x_min,x_max,y_min,y_max ranges: 79.734375 1440.0 106.3125 952.875\n",
      "Saved inference_result.jpg\n"
     ]
    }
   ],
   "source": [
    "# ---------- forward ----------\n",
    "with torch.no_grad():\n",
    "    out = model(img_tensor)\n",
    "    if isinstance(out, (list, tuple)):\n",
    "        out = out[0]   # repo sometimes returns (pred, loss) or similar\n",
    "\n",
    "# ensure out shape is [B, C, anchors] as util.non_max_suppression expects\n",
    "print(\"raw output shape:\", out.shape)\n",
    "\n",
    "# ---------- NMS (repo util) ----------\n",
    "# non_max_suppression expects outputs as-is (no permute)\n",
    "dets = non_max_suppression(out, confidence_threshold=CONF_THR, iou_threshold=IOU_THR)[0]\n",
    "\n",
    "if dets is None or len(dets) == 0:\n",
    "    print(\"No detections found.\")\n",
    "else:\n",
    "    # debug before scaling\n",
    "    d = dets.detach().cpu().clone()\n",
    "    print(\"Before scale: x_min, y_min, x_max, y_max ranges:\",\n",
    "          d[:, 0].min().item(), d[:, 1].min().item(), d[:, 2].max().item(), d[:, 3].max().item())\n",
    "    print(\"Sample raw detection (first 5):\")\n",
    "    for i in range(min(5, d.shape[0])):\n",
    "        print(i, d[i].numpy())\n",
    "\n",
    "    # ---------- map boxes from padded input -> original image ----------\n",
    "    # dets format: [x1, y1, x2, y2, conf, cls]\n",
    "    dets = dets.detach().cpu()\n",
    "    # remove padding\n",
    "    dets[:, [0, 2]] -= pad_w\n",
    "    dets[:, [1, 3]] -= pad_h\n",
    "    # divide by gain (scale)\n",
    "    dets[:, :4] /= gain\n",
    "    # clip to image size\n",
    "    dets[:, [0, 2]] = dets[:, [0, 2]].clamp(0, orig_w)\n",
    "    dets[:, [1, 3]] = dets[:, [1, 3]].clamp(0, orig_h)\n",
    "\n",
    "    # debug after scaling\n",
    "    print(\"After scale (to original image): x_min,x_max,y_min,y_max ranges:\",\n",
    "          dets[:,0].min().item(), dets[:,2].max().item(), dets[:,1].min().item(), dets[:,3].max().item())\n",
    "\n",
    "    # ---------- draw ----------\n",
    "    img_out = img_bgr.copy()  # BGR\n",
    "    \n",
    "        # BGR colors\n",
    "    CLASS_COLORS = {\n",
    "        \"person\": (0, 0, 255),        # red\n",
    "        \"motorcycle\": (0, 255, 255),  # yellow\n",
    "        \"car\": (255, 255, 255),       # white\n",
    "    }\n",
    "\n",
    "    default_color = (0, 125, 0)  # green for all other classes\n",
    "    \n",
    "    for *xyxy, conf, cls in dets:\n",
    "        x1, y1, x2, y2 = map(int, xyxy)\n",
    "        cls_id = int(cls.item())\n",
    "        cls_name = names.get(cls_id, str(cls_id))  # fallback to id if not found\n",
    "        label = f\"{cls_name} {float(conf):.2f}\"\n",
    "        \n",
    "        cls_id = int(cls.item())\n",
    "        cls_name = names.get(cls_id, str(cls_id))  # get class name from YAML\n",
    "\n",
    "        border_color = CLASS_COLORS.get(cls_name, default_color)\n",
    "\n",
    "        cv2.rectangle(img_out, (x1, y1), (x2, y2), border_color, 2)\n",
    "        cv2.putText(img_out, f\"{cls_name} {conf:.2f}\", (x1, max(y1-6,0)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, border_color, 2)\n",
    "\n",
    "\n",
    "    cv2.imwrite(\"inference_result.jpg\", img_out)\n",
    "    print(\"Saved inference_result.jpg\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
