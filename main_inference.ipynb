{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c94d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] The specified module could not be found. Error loading \"c:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\python\\machine_learning\\deep_learning\\computer vision\\YOLOv11-pt\\venv\\Lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m non_max_suppression\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myaml\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\python\\machine_learning\\deep_learning\\computer vision\\YOLOv11-pt\\venv\\Lib\\site-packages\\torch\\__init__.py:262\u001b[39m\n\u001b[32m    258\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    260\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\python\\machine_learning\\deep_learning\\computer vision\\YOLOv11-pt\\venv\\Lib\\site-packages\\torch\\__init__.py:258\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    254\u001b[39m             err = ctypes.WinError(ctypes.get_last_error())\n\u001b[32m    255\u001b[39m             err.strerror += (\n\u001b[32m    256\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    260\u001b[39m kernel32.SetErrorMode(prev_error_mode)\n",
      "\u001b[31mOSError\u001b[39m: [WinError 126] The specified module could not be found. Error loading \"c:\\Users\\TDV\\OneDrive - ut.edu.vn\\Documents\\python\\machine_learning\\deep_learning\\computer vision\\YOLOv11-pt\\venv\\Lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch, cv2\n",
    "from utils.util import non_max_suppression\n",
    "\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed19d91f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (717537615.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython -m venv venv\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- params ----------\n",
    "WEIGHT = \"weights/best.pt\"\n",
    "IMG_PATH = [\"dataset/Highway.png\", \"dataset/VietNam_street.png\"]\n",
    "DATA_YAML = \"utils/args.yaml\"   # Path to dataset yaml\n",
    "INPUT_SIZE = (640, 640) \n",
    "CONF_THR = 0.25\n",
    "IOU_THR = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_YAML, \"r\") as f:\n",
    "    data_dict = yaml.safe_load(f)\n",
    "\n",
    "names = data_dict[\"names\"]   # dict {0:\"person\",1:\"bicycle\",...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utils ----------\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # img: BGR numpy (H, W, C) as loaded by cv2\n",
    "    h0, w0 = img.shape[:2]\n",
    "    new_h, new_w = new_shape\n",
    "    r = min(new_h / h0, new_w / w0)\n",
    "    new_unpad_w = int(round(w0 * r))\n",
    "    new_unpad_h = int(round(h0 * r))\n",
    "    # resize\n",
    "    img_resized = cv2.resize(img, (new_unpad_w, new_unpad_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # compute padding\n",
    "    dw = new_w - new_unpad_w\n",
    "    dh = new_h - new_unpad_h\n",
    "    top = int(round(dh / 2 - 0.1))\n",
    "    bottom = int(round(dh / 2 + 0.1))\n",
    "    left = int(round(dw / 2 - 0.1))\n",
    "    right = int(round(dw / 2 + 0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right,\n",
    "                                    cv2.BORDER_CONSTANT, value=color)\n",
    "    return img_padded, r, (left, top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04361af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_23360\\1903992144.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# ---------- load model ----------\n",
    "ckpt = torch.load(WEIGHT, map_location=\"cpu\")\n",
    "if 'model' in ckpt:\n",
    "    model = ckpt['model']\n",
    "else:\n",
    "    raise RuntimeError(\"Checkpoint does not contain 'model' key.\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# prefer float for numeric stability\n",
    "if next(model.parameters()).dtype == torch.half:\n",
    "    model = model.half()\n",
    "else:\n",
    "    model = model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you already have: model, device, names, letterbox() from before\n",
    "\n",
    "def load_images_as_batch(img_paths, new_shape=(640, 640)):\n",
    "    \"\"\"\n",
    "    Loads multiple images, applies letterbox resize, and returns:\n",
    "    - batch tensor [B, 3, H, W]\n",
    "    - list of (original_h, original_w)\n",
    "    - list of (gain, (pad_w, pad_h))\n",
    "    \"\"\"\n",
    "    imgs_tensor = []\n",
    "    shapes = []\n",
    "    ratios = []\n",
    "\n",
    "    for path in img_paths:\n",
    "        img_bgr = cv2.imread(path) #np.array (H, W, 3)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "\n",
    "        orig_h, orig_w = img_bgr.shape[:2] #np.array (H, W)\n",
    "        img_pad, gain, (pad_w, pad_h) = letterbox(img_bgr, new_shape=new_shape)\n",
    "        '''\n",
    "Gọi hàm letterbox để resize ảnh về kích thước chuẩn của YOLO (INPUT_SIZE, ví dụ 640×640).\n",
    "Letterbox = resize ảnh nhưng vẫn giữ tỉ lệ khung hình (aspect ratio) → phần thừa sẽ được padding màu đen.\n",
    "Trả về:\n",
    "img_pad: ảnh sau khi resize + pad.\n",
    "gain: hệ số scale (ảnh gốc → ảnh mới).\n",
    "(pad_w, pad_h): độ pad thêm ở 2 chiều.\n",
    "Thông tin gain, pad_w, pad_h được dùng sau này để chuyển ngược bbox từ ảnh YOLO về ảnh gốc.\n",
    "'''\n",
    "        img_rgb = cv2.cvtColor(img_pad, cv2.COLOR_BGR2RGB) #Chuyển từ BGR → RGB (YOLO và PyTorch thường chuẩn hóa input thành RGB).\n",
    "        tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        '''\n",
    "torch.from_numpy(img_rgb): chuyển ảnh NumPy → Tensor PyTorch.\n",
    ".permute(2, 0, 1): đổi trục từ (H, W, C) → (C, H, W) (PyTorch format).\n",
    ".unsqueeze(0): thêm batch dimension → [1, C, H, W].\n",
    ".float() / 255.0: đổi từ uint8 (0–255) sang float32 (0–1) để mạng dễ học\n",
    "'''\n",
    "        imgs_tensor.append(tensor)\n",
    "        shapes.append((orig_h, orig_w))\n",
    "        ratios.append((gain, pad_w, pad_h))\n",
    "        \n",
    "\n",
    "    batch_tensor = torch.stack(imgs_tensor, 0)  # [B,3,H,W]\n",
    "    return batch_tensor, shapes, ratios\n",
    "\n",
    "\n",
    "def run_inference_batch(model, img_paths, conf_thres=0.25, iou_thres=0.45):\n",
    "    batch_tensor, shapes, ratios = load_images_as_batch(img_paths, new_shape=(640,640))\n",
    "    batch_tensor = batch_tensor.to(device)\n",
    "\n",
    "    if next(model.parameters()).dtype == torch.half:\n",
    "        batch_tensor = batch_tensor.half()\n",
    "        '''\n",
    "    Nếu mô hình đang ở dạng half precision (FP16) → convert input sang .half() để đồng bộ.\n",
    "    Dùng khi inference trên GPU để tăng tốc và tiết kiệm bộ nhớ.\n",
    "'''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch_tensor)\n",
    "        if isinstance(preds, (list, tuple)):\n",
    "            preds = preds[0]\n",
    "\n",
    "    detections = non_max_suppression(preds, confidence_threshold=conf_thres, iou_threshold=iou_thres)\n",
    "\n",
    "    results = []\n",
    "    for i, det in enumerate(detections):\n",
    "        orig_h, orig_w = shapes[i]\n",
    "        gain, pad_w, pad_h = ratios[i]\n",
    "\n",
    "        if det is not None and len(det):\n",
    "            det = det.clone()\n",
    "            # de-letterbox\n",
    "            det[:, [0, 2]] -= pad_w\n",
    "            det[:, [1, 3]] -= pad_h\n",
    "            det[:, :4] /= gain\n",
    "            det[:, [0, 2]] = det[:, [0, 2]].clamp(0, orig_w)\n",
    "            det[:, [1, 3]] = det[:, [1, 3]].clamp(0, orig_h)\n",
    "        results.append(det)\n",
    "    return results\n",
    "\n",
    "        # BGR colors\n",
    "CLASS_COLORS = {\n",
    "    \"person\": (0, 0, 255),        # red\n",
    "    \"motorcycle\": (0, 255, 255),  # yellow\n",
    "    \"car\": (255, 255, 255),       # white\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7306a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference_result_Highway.png\n",
      "Saved inference_result_VietNam_street.png\n"
     ]
    }
   ],
   "source": [
    "detections = run_inference_batch(model, IMG_PATH)\n",
    "\n",
    "for path, det in zip(IMG_PATH, detections):\n",
    "    img = cv2.imread(path)\n",
    "    if det is not None:\n",
    "        for *xyxy, conf, cls in det:\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cls_id = int(cls.item())\n",
    "            cls_name = names.get(cls_id, str(cls_id))\n",
    "            color = CLASS_COLORS.get(cls_name, (0,125,0))\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)\n",
    "            cv2.putText(img, f\"{cls_name} {conf:.2f}\", (x1, max(y1-6,0)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    save_path = f\"inference_result_{path.split('/')[-1]}\"\n",
    "    cv2.imwrite(save_path, img)\n",
    "    print(f\"Saved {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
