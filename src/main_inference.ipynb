{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c94d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, cv2\n",
    "from utils.util import non_max_suppression, non_max_suppression_editable, load_thresholds_from_args_yaml\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ebcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- params ----------\n",
    "WEIGHT = \"internal_assets/weights/best.pt\"\n",
    "IMG_PATH = [\"internal_assets/dataset/VietNam_street.png\",\n",
    "            \"internal_assets/dataset/Highway.png\",\n",
    "            \"internal_assets/dataset/VN_Highway.png\",\n",
    "            \"internal_assets/dataset/Result_image.jpg\"]\n",
    "DATA_YAML = \"utils/args.yaml\"   # Path to dataset yaml\n",
    "INPUT_SIZE = (640, 640) \n",
    "CONF_THR = 0.25\n",
    "IOU_THR = 0.45\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f10f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_YAML, \"r\") as f:\n",
    "    data_dict = yaml.safe_load(f)\n",
    "\n",
    "names = data_dict[\"names\"]   # dict {0:\"person\",1:\"bicycle\",...}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3da5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- utils ----------\n",
    "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # img: BGR numpy (H, W, C) as loaded by cv2\n",
    "    h0, w0 = img.shape[:2]\n",
    "    new_h, new_w = new_shape\n",
    "    r = min(new_h / h0, new_w / w0)\n",
    "    new_unpad_w = int(round(w0 * r))\n",
    "    new_unpad_h = int(round(h0 * r))\n",
    "    # resize\n",
    "    img_resized = cv2.resize(img, (new_unpad_w, new_unpad_h), interpolation=cv2.INTER_LINEAR)\n",
    "    # compute padding\n",
    "    dw = new_w - new_unpad_w\n",
    "    dh = new_h - new_unpad_h\n",
    "    top = int(round(dh / 2 - 0.1))\n",
    "    bottom = int(round(dh / 2 + 0.1))\n",
    "    left = int(round(dw / 2 - 0.1))\n",
    "    right = int(round(dw / 2 + 0.1))\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, top, bottom, left, right,\n",
    "                                    cv2.BORDER_CONSTANT, value=color)\n",
    "    return img_padded, r, (left, top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04361af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- load model ----------\n",
    "ckpt = torch.load(WEIGHT, map_location=\"cpu\", weights_only=False)\n",
    "if 'model' in ckpt:\n",
    "    model = ckpt['model']\n",
    "else:\n",
    "    raise RuntimeError(\"Checkpoint does not contain 'model' key.\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# prefer float for numeric stability\n",
    "if next(model.parameters()).dtype == torch.half:\n",
    "    model = model.half()\n",
    "else:\n",
    "    model = model.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a027fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you already have: model, device, names, letterbox() from before\n",
    "\n",
    "def load_images_as_batch(img_paths, new_shape=(640, 640)):\n",
    "    \"\"\"\n",
    "    Loads multiple images, applies letterbox resize, and returns:\n",
    "    - batch tensor [B, 3, H, W]\n",
    "    - list of (original_h, original_w)\n",
    "    - list of (gain, (pad_w, pad_h))\n",
    "    \"\"\"\n",
    "    imgs_tensor = []\n",
    "    shapes = []\n",
    "    ratios = []\n",
    "\n",
    "    for path in img_paths:\n",
    "        img_bgr = cv2.imread(path) #np.array (H, W, 3)\n",
    "        if img_bgr is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "\n",
    "        orig_h, orig_w = img_bgr.shape[:2] #np.array (H, W)\n",
    "        img_pad, gain, (pad_w, pad_h) = letterbox(img_bgr, new_shape=new_shape)\n",
    "        '''\n",
    "Gọi hàm letterbox để resize ảnh về kích thước chuẩn của YOLO (INPUT_SIZE, ví dụ 640×640).\n",
    "Letterbox = resize ảnh nhưng vẫn giữ tỉ lệ khung hình (aspect ratio) → phần thừa sẽ được padding màu đen.\n",
    "Trả về:\n",
    "img_pad: ảnh sau khi resize + pad.\n",
    "gain: hệ số scale (ảnh gốc → ảnh mới).\n",
    "(pad_w, pad_h): độ pad thêm ở 2 chiều.\n",
    "Thông tin gain, pad_w, pad_h được dùng sau này để chuyển ngược bbox từ ảnh YOLO về ảnh gốc.\n",
    "'''\n",
    "        img_rgb = cv2.cvtColor(img_pad, cv2.COLOR_BGR2RGB) #Chuyển từ BGR → RGB (YOLO và PyTorch thường chuẩn hóa input thành RGB).\n",
    "        tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        '''\n",
    "torch.from_numpy(img_rgb): chuyển ảnh NumPy → Tensor PyTorch.\n",
    ".permute(2, 0, 1): đổi trục từ (H, W, C) → (C, H, W) (PyTorch format).\n",
    ".unsqueeze(0): thêm batch dimension → [1, C, H, W].\n",
    ".float() / 255.0: đổi từ uint8 (0–255) sang float32 (0–1) để mạng dễ học\n",
    "'''\n",
    "        imgs_tensor.append(tensor)\n",
    "        shapes.append((orig_h, orig_w))\n",
    "        ratios.append((gain, pad_w, pad_h))\n",
    "        \n",
    "\n",
    "    batch_tensor = torch.stack(imgs_tensor, 0)  # [B,3,H,W]\n",
    "    return batch_tensor, shapes, ratios\n",
    "\n",
    "\n",
    "def run_inference_batch(model, img_paths, conf_thres=0.25, iou_thres=0.45):\n",
    "    batch_tensor, shapes, ratios = load_images_as_batch(img_paths, new_shape=(640,640))\n",
    "    batch_tensor = batch_tensor.to(device)\n",
    "\n",
    "    if next(model.parameters()).dtype == torch.half:\n",
    "        batch_tensor = batch_tensor.half()\n",
    "        '''\n",
    "    Nếu mô hình đang ở dạng half precision (FP16) → convert input sang .half() để đồng bộ.\n",
    "    Dùng khi inference trên GPU để tăng tốc và tiết kiệm bộ nhớ.\n",
    "'''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(batch_tensor)\n",
    "        if isinstance(preds, (list, tuple)):\n",
    "            preds = preds[0]\n",
    "\n",
    "    # detections = non_max_suppression(preds, confidence_threshold=conf_thres, iou_threshold=iou_thres)\n",
    "    \n",
    "    per_class_dict, per_class_list, names = load_thresholds_from_args_yaml('utils/args.yaml', default_threshold=CONF_THR)\n",
    "\n",
    "    # Then call your NMS:\n",
    "    # Option A: pass dict (works with the NMS implementation I gave earlier)\n",
    "    detections = non_max_suppression_editable(preds, confidence_threshold=per_class_dict, iou_threshold=0.45, default_threshold=CONF_THR)\n",
    "\n",
    "    results = []\n",
    "    for i, det in enumerate(detections):\n",
    "        orig_h, orig_w = shapes[i]\n",
    "        gain, pad_w, pad_h = ratios[i]\n",
    "\n",
    "        if det is not None and len(det):\n",
    "            det = det.clone()\n",
    "            # de-letterbox\n",
    "            det[:, [0, 2]] -= pad_w\n",
    "            det[:, [1, 3]] -= pad_h\n",
    "            det[:, :4] /= gain\n",
    "            det[:, [0, 2]] = det[:, [0, 2]].clamp(0, orig_w)\n",
    "            det[:, [1, 3]] = det[:, [1, 3]].clamp(0, orig_h)\n",
    "        results.append(det)\n",
    "    return results\n",
    "\n",
    "        # BGR colors\n",
    "CLASS_COLORS = {\n",
    "    \"person\": (0, 0, 255),        # red\n",
    "    \"motorcycle\": (0, 255, 255),  # yellow\n",
    "    \"car\": (255, 255, 255),       # white\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7306a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved internal_assets/inference_result/inference_result_VietNam_street.png\n",
      "Saved internal_assets/inference_result/inference_result_Highway.png\n",
      "Saved internal_assets/inference_result/inference_result_VN_Highway.png\n",
      "Saved internal_assets/inference_result/inference_result_Result_image.jpg\n"
     ]
    }
   ],
   "source": [
    "detections = run_inference_batch(model, IMG_PATH)\n",
    "\n",
    "for path, det in zip(IMG_PATH, detections):\n",
    "    img = cv2.imread(path)\n",
    "    if det is not None:\n",
    "        for *xyxy, conf, cls in det:\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            cls_id = int(cls.item())\n",
    "            cls_name = names.get(cls_id, str(cls_id))\n",
    "            color = CLASS_COLORS.get(cls_name, (0,125,0))\n",
    "            cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)\n",
    "            cv2.putText(img, f\"{cls_name} {conf:.2f}\", (x1, max(y1-6,0)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "    save_path = f\"internal_assets/inference_result/inference_result_{path.split('/')[-1]}\"\n",
    "    cv2.imwrite(save_path, img)\n",
    "    print(f\"Saved {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
