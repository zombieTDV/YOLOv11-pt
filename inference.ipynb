{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "145e39db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import importlib\n",
    "import inspect\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from utils.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439a774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = \"weights/best.pt\"\n",
    "IMG_PATH = \"dataset/VietNam_street.png\"\n",
    "#VietNam_street\n",
    "#Highway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb93cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint type: <class 'dict'>\n",
      "checkpoint keys: dict_keys(['epoch', 'model'])\n",
      "Loaded nn.Module directly from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TDV\\AppData\\Local\\Temp\\ipykernel_20460\\1134683156.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(WEIGHT_PATH, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "ckpt = torch.load(WEIGHT_PATH, map_location=\"cpu\")\n",
    "\n",
    "# check what keys exist\n",
    "print(\"checkpoint type:\", type(ckpt))\n",
    "print(\"checkpoint keys:\", ckpt.keys())\n",
    "\n",
    "# extract the model\n",
    "if \"model\" in ckpt:\n",
    "    model = ckpt[\"model\"]\n",
    "    print(\"Loaded nn.Module directly from checkpoint.\")\n",
    "else:\n",
    "    raise ValueError(\"No 'model' key in checkpoint. Found keys: \", ckpt.keys())\n",
    "\n",
    "# move to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1a667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model param dtype: torch.float16\n",
      "Converting input to half (fp16) to match model.\n",
      "output.shape: torch.Size([1, 84, 8400])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# assume 'model' is already loaded (the nn.Module from your checkpoint)\n",
    "model.to(device).eval()\n",
    "\n",
    "# show dtype of model params\n",
    "param_dtype = next(model.parameters()).dtype\n",
    "print(\"Model param dtype:\", param_dtype)\n",
    "\n",
    "# preprocess image (adapt resize if repo expects different)\n",
    "img = Image.open(IMG_PATH).convert(\"RGB\")\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 640)),\n",
    "    T.ToTensor(),        # yields float32 in [0,1]\n",
    "])\n",
    "x = transform(img).unsqueeze(0)   # shape [1,3,H,W]\n",
    "\n",
    "# move & match dtype\n",
    "x = x.to(device)\n",
    "if param_dtype == torch.half:\n",
    "    print(\"Converting input to half (fp16) to match model.\")\n",
    "    x = x.half()\n",
    "else:\n",
    "    x = x.float()\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(x)   # forward\n",
    "\n",
    "# move outputs to cpu and float for postprocessing/printing\n",
    "def to_cpu_float(t):\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        return t.detach().cpu().float()\n",
    "    return t\n",
    "\n",
    "# inspect output\n",
    "if isinstance(out, torch.Tensor):\n",
    "    print(\"output.shape:\", to_cpu_float(out).shape)\n",
    "elif isinstance(out, (list, tuple)):\n",
    "    for i,o in enumerate(out):\n",
    "        if isinstance(o, torch.Tensor):\n",
    "            print(i, to_cpu_float(o).shape)\n",
    "        else:\n",
    "            print(i, type(o))\n",
    "else:\n",
    "    print(type(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a125fc",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0476a35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved inference_result.jpg with detections.\n"
     ]
    }
   ],
   "source": [
    "# --- Load image ---\n",
    "img = Image.open(IMG_PATH).convert(\"RGB\")\n",
    "\n",
    "# repo usually expects [B,C,H,W] normalized to 0-1, size 640x640\n",
    "transform = T.Compose([\n",
    "    T.Resize((640, 640)),\n",
    "    T.ToTensor(),   # converts to float32 [0,1]\n",
    "])\n",
    "x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Match dtype\n",
    "if next(model.parameters()).dtype == torch.half:\n",
    "    x = x.half()\n",
    "\n",
    "# --- Forward pass ---\n",
    "with torch.no_grad():\n",
    "    pred = model(x)[0]   # usually model returns (pred, loss); we take pred\n",
    "\n",
    "# --- Postprocess ---\n",
    "# Apply NMS: filter boxes by confidence/IoU\n",
    "detections = non_max_suppression(pred, confidence_threshold=0.25, iou_threshold=0.45)[0]\n",
    "\n",
    "if detections is not None and len(detections):\n",
    "    # Rescale boxes from 640x640 back to original image size\n",
    "    detections[:, :4] = scale_coords(x.shape[2:], detections[:, :4], img.size[::-1]).round()\n",
    "\n",
    "    # Draw boxes with OpenCV\n",
    "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    for *xyxy, conf, cls in detections:\n",
    "        label = f\"{int(cls)} {conf:.2f}\"\n",
    "        xyxy = [int(v) for v in xyxy]\n",
    "        cv2.rectangle(img_cv, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0,255,0), 2)\n",
    "        cv2.putText(img_cv, label, (xyxy[0], xyxy[1]-2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "    cv2.imwrite(\"inference_result.jpg\", img_cv)\n",
    "    print(\"Saved inference_result.jpg with detections.\")\n",
    "else:\n",
    "    print(\"No detections found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d025bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 84, 8400])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(type(out))\n",
    "if isinstance(out, (list,tuple)):\n",
    "    for i,o in enumerate(out):\n",
    "        print(i, type(o), getattr(o,\"shape\",None))\n",
    "else:\n",
    "    print(getattr(out,\"shape\",None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
